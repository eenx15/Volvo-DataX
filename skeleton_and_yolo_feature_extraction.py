# coding=utf-8
import argparse
import math

import sys
sys.path.append('tf-pose-estimation')
#tfposeestimation =__import__('tf-pose-estimation')
from tf_pose.estimator import TfPoseEstimator
from tf_pose.networks import get_graph_path, model_wh

from collections import defaultdict
import dill
import os

from tools.generate_detections import create_box_encoder
from application_util import preprocessing
from deep_sort import nn_matching
from deep_sort.detection import Detection
from deep_sort.tracker import Tracker

from absl import app, logging, flags
from absl.flags import FLAGS
import time
import cv2
import numpy as np
import tensorflow as tf
from yolo.yolov3_tf2.models import (
    YoloV3, YoloV3Tiny
)
from yolo.yolov3_tf2.dataset import transform_images, load_tfrecord_dataset

fps_time = 0

skeletonFeatures = defaultdict(lambda: defaultdict(lambda : defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: np.array)))))

# Parameters:
nms_max_overlap = 1.0
max_cosine_distance = 0.2
nn_budget = None

# Initializes tracker encoder
modelPath = "/Users/kalle/Documents/Chalmers/Kandidatarbete/tf-pose-estimation/mars-small128.pb"
encoder = create_box_encoder(modelPath, batch_size=32)

metric = nn_matching.NearestNeighborDistanceMetric("cosine", max_cosine_distance, nn_budget)
tracker = Tracker(metric)
# YOLO settings
flags.DEFINE_string('classes', './data/own_data/coco.names', 'path to classes file')
flags.DEFINE_string('weights', './data/checkpoints/yolov3.tf','path to weights file')
flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')
flags.DEFINE_integer('size', 416, 'resize images to')
flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')
flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')
app._run_init(['yolov3'], app.parse_flags_with_usage)

FLAGS.num_classes = 80 #Run this
FLAGS.classes = './data/original/coco.names'
FLAGS.weights = './data/checkpoints/yolov3.tf'


yolo = YoloV3(classes=FLAGS.num_classes)
yolo.load_weights('./data/checkpoints/yolov3.tf').expect_partial()

class_names = [c.strip() for c in open('./data/original/coco.names').readlines()]


def str2bool(v):
    return v.lower() in ("yes", "true", "t", "1")


def run(video_path, model):
    global boxes
    resize_out_ratio = 4.0
    fps_time = 0

    #Extract name of videofile, without extension
    video_id = video_path.split('/')[-1]
    video_id = video_id.split('.')[0]

    cam = cv2.VideoCapture(video_path)

    while cam.isOpened():
        # Read next frame in video
        ret_val, image = cam.read()

        if not ret_val or image is None:
            break


        currFrame = int(cam.get(cv2.CAP_PROP_POS_FRAMES))
        fps = cam.get(cv2.CAP_PROP_FPS)

        img_in = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        img_in = tf.expand_dims(img_in, 0)
        img_in = transform_images(img_in, FLAGS.size)

        # Run YOLO on frame
        boxes, scores, classes, nums = yolo(img_in)

        # Only care about persons
        boxes, scores, classes, nums = boxes[0], scores[0], classes[0], nums[0]

        wh = np.flip(image.shape[0:2])
        bbtlwh = []
        # The list bbtlwh will contain bounding boxes of pedestrians generated by YOLO
        for i in range(nums):
            if int(classes[i]) == 0:
                x1y1 = tuple((np.array(boxes[i][0:2]) * wh).astype(np.int32))
                x1 = x1y1[0]
                y1 = x1y1[1]
                x2y2 = tuple((np.array(boxes[i][2:4]) * wh).astype(np.int32))
                bbwh = (x2y2[0]-x1y1[0], x2y2[1]-x1y1[1])
                w = bbwh[0]
                h = bbwh[1]
                bbtlwh.append([x1,y1,w,h])

        # Feed DeepSORT with detected bounding boxes
        features = encoder(image, bbtlwh)
        detections = [Detection(box, conf, feat) for box, conf, feat in zip(bbtlwh, scores, features)]

        bbox_coords = np.array([d.tlwh for d in detections])
        scores = np.array([d.confidence for d in detections])
        indices = preprocessing.non_max_suppression(
            bbox_coords, nms_max_overlap, scores)
        detections = [detections[i] for i in indices]

        # Update tracker.
        tracker.predict()
        tracker.update(detections)

        tracked_bbox = []
        ids = []

        # Loop through all tracker ids
        for track in tracker.tracks:
            if not track.is_confirmed() or track.time_since_update > 1:
                continue
            currentBB = track.to_tlwh()

            x, y, w, h = currentBB
            midX = x+w/2
            midY = y+h/2

            # The skeleton fitting generated bad skeletons when feeded with only the cropped YOLO image. Make it 50% larger.
            w = int(w * 1.5)
            h = int(h * 1.5)

            x = midX - w/2
            y = midY - h/2

            x = int(x)
            y = int(y)
            w = int(w)
            h = int(h)

            cropped = image[y:y + h, x:x + w]

            # Crop the image according to tracked YOLO BBs, and apply skeleton fitting
            if cropped.size != 0:
                cv2.rectangle(image, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 0), 2)
                humans = model.inference(cropped, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)
                # If several skeletons are fitted in the same BB, only consider the one with highest confidence. This
                # is done by sorting the list of humans.
                humans.sort(key=lambda human: human.score, reverse=True)
                skelett = TfPoseEstimator.draw_humans(cropped, humans, imgcopy=True)
                if humans:
                    # Generate features from the skeleton with highest confidence
                    create_features(humans[0], outputFeatures=skeletonFeatures[video_id][track.track_id],
                                frameNr=currFrame,
                                bounding_box=currentBB, fps=fps)
                image[y:y+h, x:x+w] = skelett

            tracked_bbox.append(track.to_tlwh())
            ids.append(track.track_id)

            # Generate image
        for i in range(len(tracked_bbox)):
            # Show tracker output
            x, y, w, h = tracked_bbox[i]
            x = int(x)
            y = int(y)
            w = int(w)
            h = int(h)
            color = (255, 0, 0)
            cv2.rectangle(image, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)
            cv2.putText(image, 'TrackID ' + str(ids[i]), (int(x + w / 2 + 35), y), cv2.FONT_HERSHEY_SIMPLEX,
                        0.5, color, 2)

        cv2.putText(image,
                    "FPS: %f" % (1.0 / (time.time() - fps_time)),
                    (10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,
                    (0, 255, 0), 2)
        cv2.imshow('tf-pose-estimation result', image)
        fps_time = time.time()
        if cv2.waitKey(1) == 27:
            break
        # Done with frame

    cv2.destroyAllWindows()

def create_features(human, outputFeatures, frameNr,bounding_box, fps):
    """
    This function saves 396 features to a dict, given a skeleton
    :param human: An instance of the class Human, which is given by tf-pose-estimation
    :param outputFeatures: The dict to save the features into
    :param frameNr: current frame number
    :param bounding_box: bounding box coordinates, enclosing the pedestrian
    :param fps: FPS rate of the video wich the application in run on
    """

    # The 9 keypoints that features are generated from
    interestingPoints = [1, 2, 5, 8, 9, 10, 11, 12, 13]
    # The list keypointLocations will contain the coordinates of the interesting keypoints
    keypointLocations = []

    foundPoints = {}

    for key, part in human.body_parts.items():
        foundPoints[part.get_coordinates()[0]] = part

    for point in interestingPoints:
        if point in foundPoints.keys():
            keypointLocations.append(np.array(foundPoints[point].get_coordinates()[1:]))
        else:
            # If one of interestingPoints are missing, append negative coordinates
            keypointLocations.append(np.array([-1 - point, -1 - point]))

    if len(keypointLocations) > 0:
        angles3 = []
        visitedI = {}

        # This block will calculate angles between three keypoints
        for i in range(len(keypointLocations)):
            visitedI[i] = True
            for j in range(len(keypointLocations)):
                if i == j:
                    continue
                for k in range(len(keypointLocations)):
                    if k == j or k == i:
                        continue
                    if k in visitedI.keys():
                        continue

                    if keypointLocations[i][0] <= -1 or keypointLocations[j][0] <= -1 or keypointLocations[k][0] <= -1:
                        angles3.append(0.0)
                    else:
                        normI = keypointLocations[i] - keypointLocations[j]
                        normJ = keypointLocations[k] - keypointLocations[j]

                        if np.linalg.norm(normI) * np.linalg.norm(normJ) == 0:
                            angles3.append(0.0)
                        else:
                            cosVal = np.dot(normI, normJ) / (np.linalg.norm(normI) * np.linalg.norm(normJ))
                            cosVal = round(cosVal, 3)
                            angles3.append(np.arccos(cosVal))

        # This will calculate the angle between two keypoints and the horizontal plane
        angles = []
        horiz = [1, 0]
        for i in range(len(keypointLocations) - 1):
            for j in range(i + 1, len(keypointLocations)):
                if keypointLocations[i][0] <= -1 or keypointLocations[j][0] <= -1:
                    angles.append(0.0)
                else:
                    if keypointLocations[i][1] > keypointLocations[j][1]:
                        normI = keypointLocations[i] - keypointLocations[j]
                    else:
                        normI = keypointLocations[j] - keypointLocations[i]

                    if np.linalg.norm(normI) != 0:
                        cosVal = np.dot(normI, horiz) / (np.linalg.norm(normI) * np.linalg.norm(horiz))
                        angle = np.arccos(cosVal)
                    else:
                        angle = 0
                    angles.append(angle)

        # This will calculate the horizontal distance between two keypoints
        xDist = []
        for i in range(len(keypointLocations) - 1):
            for j in range(i + 1, len(keypointLocations)):
                if keypointLocations[i][0] <= -1 or keypointLocations[j][0] <= -1:
                    xDist.append(0.0)
                else:
                    xDist.append(abs(keypointLocations[i][0] - keypointLocations[j][0]))

        # This will calculate the vertical distance between two keypoints
        yDist = []
        for i in range(len(keypointLocations) - 1):
            for j in range(i + 1, len(keypointLocations)):
                if keypointLocations[i][0] <= -1 or keypointLocations[j][0] <= -1:
                    yDist.append(0.0)
                else:
                    yDist.append(abs(keypointLocations[i][1] - keypointLocations[j][1]))

        # This will calculate the 2d distance between two keypoints
        sDist = []
        for i in range(len(keypointLocations) - 1):
            for j in range(i + 1, len(keypointLocations)):
                if keypointLocations[i][0] <= -1 or keypointLocations[j][0] <= -1:
                    sDist.append(0.0)
                else:
                    sDist.append(math.sqrt((keypointLocations[i][0] - keypointLocations[j][0]) ** 2 + (
                            keypointLocations[i][1] - keypointLocations[j][1]) ** 2))


        # Save all features to the dict outputFeatures
        outputFeatures[frameNr]['angles3'] = angles3
        outputFeatures[frameNr]['angles'] = angles
        outputFeatures[frameNr]['xDist'] = xDist
        outputFeatures[frameNr]['yDist'] = yDist
        outputFeatures[frameNr]['sDist'] = sDist
        outputFeatures[frameNr]['BB'] = bounding_box
        outputFeatures[frameNr]['fps'] = fps


def parse_args():
    """ Parse command line arguments.
    """
    parser = argparse.ArgumentParser(description="pedestrian")

    parser.add_argument(
        "--video_path", help="Path to the desired video",
        default=None, required=True)

    parser.add_argument(
        "--model", help="model file",
        default='cmu', required=True)

    parser.add_argument(
        "--resize", help="resize",
        default=None, required=True)

    parser.add_argument('--tensorrt', type=str, default="False",
                        help='for tensorrt process.')

    return parser.parse_args()



if __name__ == "__main__":
    print("start")
    args = parse_args()

    w, h = model_wh(args.resize)
    if w > 0 and h > 0:
        e = TfPoseEstimator(get_graph_path(args.model), target_size=(w, h), trt_bool=str2bool(args.tensorrt))
    else:
        e = TfPoseEstimator(get_graph_path(args.model), target_size=(432, 368), trt_bool=str2bool(args.tensorrt))

    for videos in os.listdir(args.video_path):
        if videos == '.DS_Store':  # Ignore this file, which is created by default by Finder in macOS.
            continue
        run(os.path.join(args.video_path,videos), e)

    with open('skeletonFeatures_28_4', 'wb') as f:
        dill.dump(skeletonFeatures,f)